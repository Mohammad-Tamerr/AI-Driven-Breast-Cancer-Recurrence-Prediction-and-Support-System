from fastapi import APIRouter, Depends
from fastapi.responses import JSONResponse
from helpers.config import Settings, get_settings

test_router = APIRouter(
    prefix="/test",
    tags=["Test"],
)

@test_router.get("/")
async def test_basic():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ· Ù„Ù„ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù€ API ÙŠØ¹Ù…Ù„"""
    return {
        "status": "success",
        "message": "API is working! ðŸš€",
        "test": True
    }

@test_router.get("/settings")
async def test_settings(app_settings: Settings = Depends(get_settings)):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚"""
    return {
        "app_name": app_settings.APP_NAME,
        "app_version": app_settings.APP_VERSION,
        "generation_backend": app_settings.GENERATION_BACKEND,
        "embedding_backend": app_settings.EMBEDDING_BACKEND,
        "has_openai_key": bool(app_settings.OPENAI_API_KEY and app_settings.OPENAI_API_KEY.strip()),
        "has_cohere_key": bool(app_settings.COHERE_API_KEY and app_settings.COHERE_API_KEY.strip()),
        "has_gemini_key": bool(app_settings.GEMINI_API_KEY and app_settings.GEMINI_API_KEY.strip()),
    }

@test_router.post("/echo")
async def test_echo(message: dict):
    """ÙŠØ±Ø¯Ø¯ Ø£ÙŠ Ø±Ø³Ø§Ù„Ø© ØªØ±Ø³Ù„Ù‡Ø§ Ù„Ù‡"""
    return {
        "status": "success",
        "your_message": message,
        "echo": f"You said: {message.get('text', 'nothing')}",
        "timestamp": "2026-01-03"
    }

@test_router.get("/hello/{name}")
async def test_hello(name: str):
    """Ø±Ø¯ Ø´Ø®ØµÙŠ Ø¨Ø§Ù„Ø§Ø³Ù…"""
    return {
        "status": "success",
        "message": f"Hello {name}! ðŸ‘‹",
        "greeting": f"Ù…Ø±Ø­Ø¨Ø§Ù‹ {name}ØŒ Ø§Ù„Ù€ API ÙŠØ¹Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­!"
    }

@test_router.get("/gemini")
async def test_gemini_key(app_settings: Settings = Depends(get_settings)):
    """Ø§Ø®ØªØ¨Ø§Ø± ÙØ¹Ù„ÙŠ Ù„Ù€ Gemini API key"""
    
    if not app_settings.GEMINI_API_KEY:
        return {
            "status": "error",
            "message": "Gemini API key not found",
            "working": False
        }
    
    try:
        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ Gemini
        from google import genai
        
        # Ø¥Ù†Ø´Ø§Ø¡ client
        client = genai.Client(api_key=app_settings.GEMINI_API_KEY)
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ· - Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents="Say hello in Arabic"
        )
        
        return {
            "status": "success",
            "message": "Gemini API key is working! ðŸŽ‰",
            "working": True,
            "gemini_response": response.text if hasattr(response, 'text') else str(response),
            "api_key_preview": app_settings.GEMINI_API_KEY[:10] + "..." if app_settings.GEMINI_API_KEY else None
        }
        
    except ImportError:
        return {
            "status": "error", 
            "message": "google-genai library not installed. Run: pip install google-genai",
            "working": False
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Gemini API key test failed: {str(e)}",
            "working": False,
            "error_type": type(e).__name__
        }

@test_router.get("/gemini/full")
async def test_gemini_full_performance(app_settings: Settings = Depends(get_settings)):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ø£Ø¯Ø§Ø¡ Gemini ÙÙŠ Generation Ùˆ Embedding"""
    
    if not app_settings.GEMINI_API_KEY:
        return {
            "status": "error",
            "message": "Gemini API key not found",
            "working": False
        }
    
    results = {
        "generation": {},
        "embedding": {},
        "overall": {}
    }
    
    try:
        from google import genai
        import time
        
        # Ø¥Ù†Ø´Ø§Ø¡ client
        client = genai.Client(api_key=app_settings.GEMINI_API_KEY)
        
        # =========================
        # Ø§Ø®ØªØ¨Ø§Ø± Text Generation
        # =========================
        try:
            start_time = time.time()
            
            generation_response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents="Ø§ÙƒØªØ¨ ÙÙ‚Ø±Ø© Ù‚ØµÙŠØ±Ø© Ø¹Ù† Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø·Ø¨"
            )
            
            generation_time = time.time() - start_time
            
            results["generation"] = {
                "status": "success",
                "working": True,
                "response_time_seconds": round(generation_time, 3),
                "model_used": "gemini-2.5-flash",
                "response_length": len(generation_response.text) if hasattr(generation_response, 'text') else 0,
                "sample_response": (generation_response.text[:200] + "...") if hasattr(generation_response, 'text') and len(generation_response.text) > 200 else getattr(generation_response, 'text', 'No text response'),
                "performance": "fast" if generation_time < 2 else "medium" if generation_time < 5 else "slow"
            }
            
        except Exception as e:
            results["generation"] = {
                "status": "error", 
                "working": False,
                "error": str(e),
                "error_type": type(e).__name__
            }
        
        # =========================
        # Ø§Ø®ØªØ¨Ø§Ø± Text Embedding
        # =========================
        try:
            start_time = time.time()
            
            # Ù†ØµÙˆØµ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
            test_texts = [
                "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ ØªØ´Ø®ÙŠØµ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶",
                "Machine learning improves medical diagnosis", 
                "Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©"
            ]
            
            embedding_response = client.models.embed_content(
                model="text-embedding-004",
                contents=test_texts[0]
            )
            
            embedding_time = time.time() - start_time
            
            # ØªØ¬Ø±Ø¨Ø© embedding Ù…ØªØ¹Ø¯Ø¯
            start_time_batch = time.time()
            batch_responses = []
            for text in test_texts:
                batch_response = client.models.embed_content(
                    model="text-embedding-004", 
                    contents=text
                )
                batch_responses.append(batch_response)
            
            batch_time = time.time() - start_time_batch
            
            results["embedding"] = {
                "status": "success",
                "working": True,
                "single_embedding": {
                    "response_time_seconds": round(embedding_time, 3),
                    "model_used": "text-embedding-004",
                    "embedding_dimension": len(embedding_response.embedding) if hasattr(embedding_response, 'embedding') else 0,
                    "performance": "fast" if embedding_time < 1 else "medium" if embedding_time < 3 else "slow"
                },
                "batch_embedding": {
                    "texts_count": len(test_texts),
                    "total_time_seconds": round(batch_time, 3),
                    "average_time_per_text": round(batch_time / len(test_texts), 3),
                    "performance": "fast" if batch_time < 3 else "medium" if batch_time < 8 else "slow"
                },
                "embedding_preview": embedding_response.embedding[:5] if hasattr(embedding_response, 'embedding') else "No embedding data"
            }
            
        except Exception as e:
            results["embedding"] = {
                "status": "error",
                "working": False, 
                "error": str(e),
                "error_type": type(e).__name__
            }
        
        # =========================
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
        # =========================
        working_services = []
        if results["generation"].get("working", False):
            working_services.append("generation")
        if results["embedding"].get("working", False):
            working_services.append("embedding")
        
        overall_performance = "excellent"
        if results["generation"].get("working") and results["embedding"].get("working"):
            gen_perf = results["generation"].get("performance", "slow")
            emb_perf = results["embedding"]["single_embedding"].get("performance", "slow") 
            
            if "slow" in [gen_perf, emb_perf]:
                overall_performance = "good"
            elif "medium" in [gen_perf, emb_perf]:
                overall_performance = "very_good"
        elif len(working_services) == 1:
            overall_performance = "partial"
        else:
            overall_performance = "poor"
        
        results["overall"] = {
            "api_key_status": "working",
            "services_working": working_services,
            "services_count": f"{len(working_services)}/2",
            "overall_performance": overall_performance,
            "recommendation": "API ready for production" if len(working_services) == 2 else "Partial functionality - check errors"
        }
        
        return {
            "status": "success",
            "message": f"Gemini performance test completed! {len(working_services)}/2 services working",
            "api_key_preview": app_settings.GEMINI_API_KEY[:10] + "..." if app_settings.GEMINI_API_KEY else None,
            "test_results": results
        }
        
    except ImportError:
        return {
            "status": "error",
            "message": "google-genai library not installed. Run: pip install google-genai",
            "working": False
        }
    except Exception as e:
        return {
            "status": "error", 
            "message": f"Gemini full test failed: {str(e)}",
            "working": False,
            "error_type": type(e).__name__
        }

@test_router.post("/chat")
async def chat_with_gemini(message: dict, app_settings: Settings = Depends(get_settings)):
    """Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Gemini - Ø´Ø§Øª Ø¨ÙˆØª Ø¨Ø³ÙŠØ·"""
    
    if not app_settings.GEMINI_API_KEY:
        return {
            "status": "error",
            "message": "Gemini API key not found",
            "response": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø®Ø¯Ù…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ø§Ù„Ø¢Ù†"
        }
    
    user_message = message.get("message", "").strip()
    if not user_message:
        return {
            "status": "error", 
            "message": "No message provided",
            "response": "Ù…Ù† ÙØ¶Ù„Ùƒ Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„Ø©!"
        }
    
    try:
        from google import genai
        import time
        
        # Ø¥Ù†Ø´Ø§Ø¡ client
        client = genai.Client(api_key=app_settings.GEMINI_API_KEY)
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù€ prompt Ù„Ù„Ø´Ø§Øª Ø¨ÙˆØª
        enhanced_prompt = f"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ØŒ ØªØ¬ÙŠØ¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙˆØ¯ÙˆØ¯Ø© ÙˆÙ…ÙÙŠØ¯Ø©.
        
Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_message}

ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø©:
- ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…ÙÙŠØ¯Ø©
- Ù…Ù‡Ø°Ø¨Ø© ÙˆÙˆØ¯ÙˆØ¯Ø©  
- Ù…Ø®ØªØµØ±Ø© Ù„ÙƒÙ† Ø´Ø§Ù…Ù„Ø©
- Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
"""
        
        start_time = time.time()
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=enhanced_prompt
        )
        
        response_time = time.time() - start_time
        
        bot_response = response.text if hasattr(response, 'text') else "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ÙÙ‡Ù… Ø·Ù„Ø¨Ùƒ"
        
        return {
            "status": "success",
            "user_message": user_message,
            "bot_response": bot_response,
            "response_time": round(response_time, 2),
            "model": "gemini-2.5-flash",
            "timestamp": "2026-01-03"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"Chat failed: {str(e)}",
            "response": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø®Ø¯Ù…Ø©",
            "user_message": user_message
        }

@test_router.post("/chat/medical")
async def medical_chat_with_gemini(message: dict, app_settings: Settings = Depends(get_settings)):
    """Ù…Ø­Ø§Ø¯Ø«Ø© Ø·Ø¨ÙŠØ© Ù…ØªØ®ØµØµØ© Ù…Ø¹ Gemini"""
    
    if not app_settings.GEMINI_API_KEY:
        return {
            "status": "error",
            "message": "Gemini API key not found",
            "response": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù„Ø®Ø¯Ù…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© Ø§Ù„Ø¢Ù†"
        }
    
    user_message = message.get("message", "").strip()
    if not user_message:
        return {
            "status": "error",
            "message": "No message provided", 
            "response": "Ù…Ù† ÙØ¶Ù„Ùƒ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø·Ø¨ÙŠ!"
        }
    
    try:
        from google import genai
        import time
        
        client = genai.Client(api_key=app_settings.GEMINI_API_KEY)
        
        # prompt Ù…ØªØ®ØµØµ Ù„Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©
        medical_prompt = f"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ØµØ­Ø© ÙˆØ³Ø±Ø·Ø§Ù† Ø§Ù„Ø«Ø¯ÙŠ.

Ù…Ù‡Ø§Ù…:
- ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø·Ø¨ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙÙŠØ¯Ø©
- Ø§Ù„ØªÙˆØ¹ÙŠØ© Ø¨Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ÙØ­Øµ Ø§Ù„Ù…Ø¨ÙƒØ±
- ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ§Ù„Ù…Ø¹Ù†ÙˆÙŠ
- Ø§Ù„ØªØ°ÙƒÙŠØ± Ø¨Ø£Ù‡Ù…ÙŠØ© Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø®ØªØµ

ØªÙ†Ø¨ÙŠÙ‡ Ù…Ù‡Ù…: Ø§Ø°ÙƒØ± Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø£Ù† Ù‡Ø°Ù‡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙ„ÙŠØ³Øª Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ©.

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø±ÙŠØ¶: {user_message}

ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø·Ø±ÙŠÙ‚Ø©:
- Ø¹Ù„Ù…ÙŠØ© ÙˆØ¯Ù‚ÙŠÙ‚Ø©
- Ù…Ø·Ù…Ø¦Ù†Ø© ÙˆÙ…Ø´Ø¬Ø¹Ø©
- ÙˆØ§Ø¶Ø­Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù…
- Ù…Ø¹ Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨
"""
        
        start_time = time.time()
        
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=medical_prompt
        )
        
        response_time = time.time() - start_time
        
        bot_response = response.text if hasattr(response, 'text') else "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ø·Ø¨ÙŠ"
        
        return {
            "status": "success",
            "user_message": user_message,
            "medical_response": bot_response,
            "response_time": round(response_time, 2),
            "model": "gemini-2.5-flash",
            "disclaimer": "Ù‡Ø°Ù‡ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ø§Ù…Ø© ÙˆÙ„ÙŠØ³Øª Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ø¹Ù† Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ®ØµØµØ©",
            "timestamp": "2026-01-03"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"Medical chat failed: {str(e)}",
            "response": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø·Ø¨ÙŠØ©",
            "user_message": user_message
        }

@test_router.get("/all-apis")
async def test_all_api_keys(app_settings: Settings = Depends(get_settings)):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù€ API Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©"""
    results = {}
    
    # Ø§Ø®ØªØ¨Ø§Ø± Gemini
    try:
        if app_settings.GEMINI_API_KEY:
            from google import genai
            client = genai.Client(api_key=app_settings.GEMINI_API_KEY)
            response = client.models.generate_content(
                model="gemini-2.5-flash",
                contents="Say hello"
            )
            results["gemini"] = {"status": "success", "working": True, "response": "Hello from Gemini!"}
        else:
            results["gemini"] = {"status": "error", "working": False, "message": "API key not found"}
    except Exception as e:
        results["gemini"] = {"status": "error", "working": False, "message": str(e)}
    
    # Ø§Ø®ØªØ¨Ø§Ø± OpenAI
    try:
        if app_settings.OPENAI_API_KEY:
            from openai import OpenAI
            client = OpenAI(api_key=app_settings.OPENAI_API_KEY)
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": "Say hello"}],
                max_tokens=10
            )
            results["openai"] = {"status": "success", "working": True, "response": "Hello from OpenAI!"}
        else:
            results["openai"] = {"status": "error", "working": False, "message": "API key not found"}
    except Exception as e:
        results["openai"] = {"status": "error", "working": False, "message": str(e)}
    
    # Ø§Ø®ØªØ¨Ø§Ø± Cohere
    try:
        if app_settings.COHERE_API_KEY:
            import cohere
            client = cohere.Client(api_key=app_settings.COHERE_API_KEY)
            response = client.generate(prompt="Say hello", max_tokens=10)
            results["cohere"] = {"status": "success", "working": True, "response": "Hello from Cohere!"}
        else:
            results["cohere"] = {"status": "error", "working": False, "message": "API key not found"}
    except Exception as e:
        results["cohere"] = {"status": "error", "working": False, "message": str(e)}
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
    working_count = sum(1 for result in results.values() if result.get("working", False))
    total_count = len(results)
    
    return {
        "status": "success",
        "message": f"API test completed: {working_count}/{total_count} APIs working",
        "summary": {
            "total_apis": total_count,
            "working_apis": working_count,
            "success_rate": f"{(working_count/total_count*100):.1f}%"
        },
        "details": results
    }

@test_router.get("/openai")
async def test_openai_key(app_settings: Settings = Depends(get_settings)):
    """Ø§Ø®ØªØ¨Ø§Ø± ÙØ¹Ù„ÙŠ Ù„Ù€ OpenAI API key"""
    
    if not app_settings.OPENAI_API_KEY:
        return {
            "status": "error",
            "message": "OpenAI API key not found",
            "working": False
        }
    
    try:
        from openai import OpenAI
        client = OpenAI(api_key=app_settings.OPENAI_API_KEY)
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Say hello in Arabic"}],
            max_tokens=50
        )
        
        return {
            "status": "success", 
            "message": "OpenAI API key is working! ðŸŽ‰",
            "working": True,
            "openai_response": response.choices[0].message.content,
            "api_key_preview": app_settings.OPENAI_API_KEY[:10] + "..." if app_settings.OPENAI_API_KEY else None
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"OpenAI API key test failed: {str(e)}",
            "working": False,
            "error_type": type(e).__name__
        }

@test_router.get("/cohere") 
async def test_cohere_key(app_settings: Settings = Depends(get_settings)):
    """Ø§Ø®ØªØ¨Ø§Ø± ÙØ¹Ù„ÙŠ Ù„Ù€ Cohere API key"""
    
    if not app_settings.COHERE_API_KEY:
        return {
            "status": "error",
            "message": "Cohere API key not found", 
            "working": False
        }
    
    try:
        import cohere
        client = cohere.Client(api_key=app_settings.COHERE_API_KEY)
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
        response = client.generate(
            prompt="Say hello in Arabic",
            max_tokens=50
        )
        
        return {
            "status": "success",
            "message": "Cohere API key is working! ðŸŽ‰", 
            "working": True,
            "cohere_response": response.generations[0].text.strip(),
            "api_key_preview": app_settings.COHERE_API_KEY[:10] + "..." if app_settings.COHERE_API_KEY else None
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"Cohere API key test failed: {str(e)}",
            "working": False,
            "error_type": type(e).__name__
        }
